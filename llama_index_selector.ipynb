{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating different models from the Azure AI catalog with LlamaIndex and Phoenix\n",
    "\n",
    "The following notebook demonstrate how users can use multiple models from Azure AI studio depending on the scenario and use the right model for the right job. In this case, we will use LLamaIndex to build a RAG system and select different models from different providers, maximizing the capabilities they have on each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing\n",
    "\n",
    "In this example, we will use multiple models deployed in this project, including Phi-3, Cohere Command R+, Cohere Embed V3, Mistral Large, and OpenAI GPT-4o. Endpoints URLs and keys are stored in the `.env` file. Please update it accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure asynchronous operations on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure instrumentation\n",
    "\n",
    "We will use LlamaIndex to build a RAG system to answer different questions from the Paul Grahm dataset. To identify opportunities of improvement, we are using Phonix for tracing and monitoring. The following section configures automatic instrumentation of LlamaIndex and connects it with a Phoenix instance running locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace import SpanLimits, TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:phoenix.config:ðŸ“‹ Ensuring phoenix working directory: /Users/john0isaac/.phoenix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure tracing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = session.url + \"v1/traces\"\n",
    "tracer_provider = TracerProvider(\n",
    "    span_limits=SpanLimits(max_attributes=100_000)\n",
    ")\n",
    "tracer_provider.add_span_processor(\n",
    "    SimpleSpanProcessor(OTLPSpanExporter(endpoint))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure instrumentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a RAG system with models from the catalog\n",
    "\n",
    "Let's use the Cohere model ecosystem to implement our RAG solution. Cohere models are optimized for RAG patterns and they can work in a large range of languages, specially when using the Cohere Embed V3 Multilingual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "from llama_index.llms.azure_inference import AzureAICompletionsModel\n",
    "from llama_index.embeddings.azure_inference import AzureAIEmbeddingsModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohere Command R+:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureAICompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_AI_COHERE_CMDR_ENDPOINT_URL\"],\n",
    "    credential=os.environ[\"AZURE_AI_COHERE_CMDR_ENDPOINT_KEY\"],\n",
    "    model_name=os.environ[\"AZURE_AI_COHERE_CMDR_MODEL_NAME\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [openinference.instrumentation.llama_index._handler] Open span is missing for event.span_id='AzureAICompletionsModel.complete-30830167-55bf-4161-ad2f-37906b20648e', event.id_=UUID('022bec49-437b-4d77-8295-7458a555800b')\n",
      "WARNI [openinference.instrumentation.llama_index._handler] Open span is missing for event.span_id='AzureAICompletionsModel.chat-26abf0e9-35ef-4c37-a98a-252891915713', event.id_=UUID('cbc4d1ff-fa22-442e-850e-15cd529a7cc0')\n",
      "WARNI [openinference.instrumentation.llama_index._handler] Open span is missing for event.span_id='AzureAICompletionsModel.chat-26abf0e9-35ef-4c37-a98a-252891915713', event.id_=UUID('9b96fd83-e84e-4e03-b7c1-94b06295f869')\n",
      "WARNI [openinference.instrumentation.llama_index._handler] Open span is missing for id_='AzureAICompletionsModel.chat-26abf0e9-35ef-4c37-a98a-252891915713'\n",
      "WARNI [openinference.instrumentation.llama_index._handler] Open span is missing for event.span_id='AzureAICompletionsModel.complete-30830167-55bf-4161-ad2f-37906b20648e', event.id_=UUID('34d10d10-6703-4219-bf2b-11e06bb1d2d0')\n",
      "WARNI [openinference.instrumentation.llama_index._handler] Open span is missing for id_='AzureAICompletionsModel.complete-30830167-55bf-4161-ad2f-37906b20648e'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sun is shining brightly, creating a picturesque scene. The gentle breeze carries a hint of freshness, inviting you to take a deep breath and embrace the beauty of the day. It's a perfect moment to appreciate nature's palette and the calming tranquility it offers.\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "response = llm.complete(\"The sky is a beautiful blue and\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohere Embed V3 - Multilingual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = AzureAIEmbeddingsModel(\n",
    "    endpoint=os.environ[\"AZURE_AI_COHERE_EMBED_ENDPOINT_URL\"],\n",
    "    credential=os.environ[\"AZURE_AI_COHERE_EMBED_ENDPOINT_KEY\"],\n",
    "    model_name=os.environ[\"AZURE_AI_COHERE_EMBED_MODEL_NAME\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the index\n",
    "\n",
    "To demostrate how to use different models, let's first create an index using Cohere models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use the Paul Graham dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data/paul_graham\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have documents, we create nodes by applying chunking into it as it is configured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = Settings.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize storage context, by default it's in-memory so we don't have to worry about persisting them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RAG system will be able to answer questions that look to summarize multiple sources of information vs a more simple retrieval strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary index is a simple data structure where nodes are stored in a sequence. During index construction, the document texts are chunked up, converted to nodes, and stored in a list. During query time, the summary index iterates through the nodes with some optional filter parameters, and synthesizes an answer from all the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_index = SummaryIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(no_model_name) No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header.\nCode: no_model_name\nMessage: No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summarize_query_engine \u001b[38;5;241m=\u001b[39m \u001b[43msummary_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_query_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtree_summarize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/indices/base.py:411\u001b[0m, in \u001b[0;36mBaseIndex.as_query_engine\u001b[0;34m(self, llm, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_retriever(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    405\u001b[0m llm \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    406\u001b[0m     resolve_llm(llm, callback_manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager)\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m llm_from_settings_or_context(Settings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_context)\n\u001b[1;32m    409\u001b[0m )\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRetrieverQueryEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/query_engine/retriever_query_engine.py:110\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.from_args\u001b[0;34m(cls, retriever, llm, response_synthesizer, node_postprocessors, response_mode, text_qa_template, refine_template, summary_template, simple_template, output_cls, use_async, streaming, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a RetrieverQueryEngine object.\".\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m llm_from_settings_or_context(Settings, service_context)\n\u001b[0;32m--> 110\u001b[0m response_synthesizer \u001b[38;5;241m=\u001b[39m response_synthesizer \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mget_response_synthesizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefine_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefine_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimple_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimple_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_async\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m callback_manager_from_settings_or_context(\n\u001b[1;32m    124\u001b[0m     Settings, service_context\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    128\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mretriever,\n\u001b[1;32m    129\u001b[0m     response_synthesizer\u001b[38;5;241m=\u001b[39mresponse_synthesizer,\n\u001b[1;32m    130\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[1;32m    131\u001b[0m     node_postprocessors\u001b[38;5;241m=\u001b[39mnode_postprocessors,\n\u001b[1;32m    132\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/response_synthesizers/factory.py:74\u001b[0m, in \u001b[0;36mget_response_synthesizer\u001b[0;34m(llm, prompt_helper, service_context, text_qa_template, refine_template, summary_template, simple_template, response_mode, callback_manager, use_async, streaming, structured_answer_filtering, output_cls, program_factory, verbose)\u001b[0m\n\u001b[1;32m     68\u001b[0m     prompt_helper \u001b[38;5;241m=\u001b[39m service_context\u001b[38;5;241m.\u001b[39mprompt_helper\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     prompt_helper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m         prompt_helper\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39m_prompt_helper\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m PromptHelper\u001b[38;5;241m.\u001b[39mfrom_llm_metadata(\n\u001b[0;32m---> 74\u001b[0m             \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m,\n\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_mode \u001b[38;5;241m==\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mREFINE:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Refine(\n\u001b[1;32m     80\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     81\u001b[0m         callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m         service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[1;32m     92\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/llms/azure_inference/base.py:282\u001b[0m, in \u001b[0;36mAzureAICompletionsModel.metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMMetadata:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name:\n\u001b[0;32m--> 282\u001b[0m         model_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model_info:\n\u001b[1;32m    284\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name \u001b[38;5;241m=\u001b[39m model_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/ai/inference/_patch.py:660\u001b[0m, in \u001b[0;36mChatCompletionsClient.get_model_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns information about the AI model.\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \n\u001b[1;32m    655\u001b[0m \u001b[38;5;124;03m:return: ModelInfo. The ModelInfo is compatible with MutableMapping\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m:rtype: ~azure.ai.inference.models.ModelInfo\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03m:raises ~azure.core.exceptions.HttpResponseError:\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_info:\n\u001b[0;32m--> 660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_info\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/ai/inference/_operations/_operations.py:558\u001b[0m, in \u001b[0;36mChatCompletionsClientOperationsMixin._get_model_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m         response\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# Load the body in memory and close the socket\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[0;32m--> 558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[1;32m    561\u001b[0m     deserialized \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39miter_bytes()\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: (no_model_name) No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header.\nCode: no_model_name\nMessage: No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header."
     ]
    }
   ],
   "source": [
    "summarize_query_engine = summary_index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VectorStoreIndex` only stores nodes in document store if vector store does not store text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(no_model_name) No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header.\nCode: no_model_name\nMessage: No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vector_query_engine \u001b[38;5;241m=\u001b[39m \u001b[43mvector_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_query_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/indices/base.py:411\u001b[0m, in \u001b[0;36mBaseIndex.as_query_engine\u001b[0;34m(self, llm, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_retriever(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    405\u001b[0m llm \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    406\u001b[0m     resolve_llm(llm, callback_manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager)\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m llm_from_settings_or_context(Settings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_context)\n\u001b[1;32m    409\u001b[0m )\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRetrieverQueryEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/query_engine/retriever_query_engine.py:110\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.from_args\u001b[0;34m(cls, retriever, llm, response_synthesizer, node_postprocessors, response_mode, text_qa_template, refine_template, summary_template, simple_template, output_cls, use_async, streaming, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a RetrieverQueryEngine object.\".\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m llm_from_settings_or_context(Settings, service_context)\n\u001b[0;32m--> 110\u001b[0m response_synthesizer \u001b[38;5;241m=\u001b[39m response_synthesizer \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mget_response_synthesizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefine_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefine_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimple_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimple_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_async\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m callback_manager_from_settings_or_context(\n\u001b[1;32m    124\u001b[0m     Settings, service_context\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    128\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mretriever,\n\u001b[1;32m    129\u001b[0m     response_synthesizer\u001b[38;5;241m=\u001b[39mresponse_synthesizer,\n\u001b[1;32m    130\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[1;32m    131\u001b[0m     node_postprocessors\u001b[38;5;241m=\u001b[39mnode_postprocessors,\n\u001b[1;32m    132\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/response_synthesizers/factory.py:74\u001b[0m, in \u001b[0;36mget_response_synthesizer\u001b[0;34m(llm, prompt_helper, service_context, text_qa_template, refine_template, summary_template, simple_template, response_mode, callback_manager, use_async, streaming, structured_answer_filtering, output_cls, program_factory, verbose)\u001b[0m\n\u001b[1;32m     68\u001b[0m     prompt_helper \u001b[38;5;241m=\u001b[39m service_context\u001b[38;5;241m.\u001b[39mprompt_helper\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     prompt_helper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m         prompt_helper\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39m_prompt_helper\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m PromptHelper\u001b[38;5;241m.\u001b[39mfrom_llm_metadata(\n\u001b[0;32m---> 74\u001b[0m             \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m,\n\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_mode \u001b[38;5;241m==\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mREFINE:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Refine(\n\u001b[1;32m     80\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     81\u001b[0m         callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m         service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[1;32m     92\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/llms/azure_inference/base.py:282\u001b[0m, in \u001b[0;36mAzureAICompletionsModel.metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMMetadata:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name:\n\u001b[0;32m--> 282\u001b[0m         model_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model_info:\n\u001b[1;32m    284\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name \u001b[38;5;241m=\u001b[39m model_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/ai/inference/_patch.py:660\u001b[0m, in \u001b[0;36mChatCompletionsClient.get_model_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns information about the AI model.\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \n\u001b[1;32m    655\u001b[0m \u001b[38;5;124;03m:return: ModelInfo. The ModelInfo is compatible with MutableMapping\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m:rtype: ~azure.ai.inference.models.ModelInfo\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03m:raises ~azure.core.exceptions.HttpResponseError:\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_info:\n\u001b[0;32m--> 660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_info\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/ai/inference/_operations/_operations.py:558\u001b[0m, in \u001b[0;36mChatCompletionsClientOperationsMixin._get_model_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m         response\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# Load the body in memory and close the socket\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[0;32m--> 558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[1;32m    561\u001b[0m     deserialized \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39miter_bytes()\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: (no_model_name) No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header.\nCode: no_model_name\nMessage: No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header."
     ]
    }
   ],
   "source": [
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing the query engine with the search tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summarize_query_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryEngineTool\n\u001b[1;32m      3\u001b[0m summary_tool \u001b[38;5;241m=\u001b[39m QueryEngineTool\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[0;32m----> 4\u001b[0m     query_engine\u001b[38;5;241m=\u001b[39m\u001b[43msummarize_query_engine\u001b[49m,\n\u001b[1;32m      5\u001b[0m     description\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUseful for summarization questions related to Paul Graham essay on\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m What I Worked On.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     ),\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m vector_tool \u001b[38;5;241m=\u001b[39m QueryEngineTool\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[1;32m     12\u001b[0m     query_engine\u001b[38;5;241m=\u001b[39mvector_query_engine,\n\u001b[1;32m     13\u001b[0m     description\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     ),\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summarize_query_engine' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summarize_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to Paul Graham essay on\"\n",
    "        \" What I Worked On.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from Paul Graham essay on What\"\n",
    "        \" I Worked On.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_tool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RouterQueryEngine\n\u001b[1;32m      3\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m RouterQueryEngine(\n\u001b[1;32m      4\u001b[0m     selector\u001b[38;5;241m=\u001b[39mLLMSingleSelector\u001b[38;5;241m.\u001b[39mfrom_defaults(),\n\u001b[1;32m      5\u001b[0m     query_engine_tools\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m----> 6\u001b[0m         \u001b[43msummary_tool\u001b[49m,\n\u001b[1;32m      7\u001b[0m         vector_tool,\n\u001b[1;32m      8\u001b[0m     ],\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_tool' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the summary of the document?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_engine' is not defined"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat did Paul Graham do after RICS?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_engine' is not defined"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did Paul Graham do after RICS?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an smaller model for simpler tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using exactly the same class `AzureAIModelInferenceLLM` we can instantiate another model, in this case a Phi-3-mini-4K model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "slm = AzureAICompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_AI_PHI3_MINI_ENDPOINT_URL\"],\n",
    "    credential=os.environ[\"AZURE_AI_PHI3_MINI_ENDPOINT_KEY\"],\n",
    "    model_name=os.environ[\"AZURE_AI_PHI3_MINI_MODEL_NAME\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's configure the `RouterQueryEngine` to use Phi-3 for the routing task instead of the larger model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_tool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m RouterQueryEngine(\n\u001b[1;32m      2\u001b[0m     selector\u001b[38;5;241m=\u001b[39mLLMSingleSelector\u001b[38;5;241m.\u001b[39mfrom_defaults(llm\u001b[38;5;241m=\u001b[39mslm),\n\u001b[1;32m      3\u001b[0m     query_engine_tools\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m----> 4\u001b[0m         \u001b[43msummary_tool\u001b[49m,\n\u001b[1;32m      5\u001b[0m         vector_tool,\n\u001b[1;32m      6\u001b[0m     ],\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_tool' is not defined"
     ]
    }
   ],
   "source": [
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(llm=slm),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat did Paul Graham do after RICS?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_engine' is not defined"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did Paul Graham do after RICS?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an evaluation dataset\n",
    "\n",
    "Let's build an evaluation dataset to see the effect of the change in the model. We will use another LLM to generate examples, in this case Mistral Large which is a good model for RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_llm = AzureAICompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_AI_MISTRAL_ENDPOINT_URL\"],\n",
    "    credential=os.environ[\"AZURE_AI_MISTRAL_ENDPOINT_KEY\"],\n",
    "    model_name=os.environ[\"AZURE_AI_MISTRAL_MODEL_NAME\"],\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import LabelledRagDataset\n",
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = RagDatasetGenerator.from_documents(\n",
    "    documents=documents,\n",
    "    llm=generator_llm,\n",
    "    num_questions_per_chunk=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(no_model_name) No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header.\nCode: no_model_name\nMessage: No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rag_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_questions_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/llama_dataset/generator.py:251\u001b[0m, in \u001b[0;36mRagDatasetGenerator.generate_questions_from_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_questions_from_nodes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LabelledRagDataset:\n\u001b[1;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generates questions but not the reference answers.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magenerate_questions_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/async_utils.py:33\u001b[0m, in \u001b[0;36masyncio_run\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     30\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# If we're here, there's an existing loop but it's not running\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# If we can't get the event loop, we're likely in a different thread, or its already running\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/llama_dataset/generator.py:243\u001b[0m, in \u001b[0;36mRagDatasetGenerator.agenerate_questions_from_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_questions_from_nodes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LabelledRagDataset:\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generates questions but not the reference answers.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes, labelled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/llama_dataset/generator.py:170\u001b[0m, in \u001b[0;36mRagDatasetGenerator._agenerate_dataset\u001b[0;34m(self, nodes, labelled)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n\u001b[1;32m    158\u001b[0m     index \u001b[38;5;241m=\u001b[39m SummaryIndex\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m    159\u001b[0m         [\n\u001b[1;32m    160\u001b[0m             Document(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         ],\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 170\u001b[0m     query_engine \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_query_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_question_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     task \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39maquery(\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquestion_gen_query,\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     query_tasks\u001b[38;5;241m.\u001b[39mappend(task)\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/indices/base.py:411\u001b[0m, in \u001b[0;36mBaseIndex.as_query_engine\u001b[0;34m(self, llm, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_retriever(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    405\u001b[0m llm \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    406\u001b[0m     resolve_llm(llm, callback_manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager)\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m llm_from_settings_or_context(Settings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_context)\n\u001b[1;32m    409\u001b[0m )\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRetrieverQueryEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/query_engine/retriever_query_engine.py:110\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.from_args\u001b[0;34m(cls, retriever, llm, response_synthesizer, node_postprocessors, response_mode, text_qa_template, refine_template, summary_template, simple_template, output_cls, use_async, streaming, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a RetrieverQueryEngine object.\".\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m llm_from_settings_or_context(Settings, service_context)\n\u001b[0;32m--> 110\u001b[0m response_synthesizer \u001b[38;5;241m=\u001b[39m response_synthesizer \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mget_response_synthesizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefine_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefine_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimple_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimple_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_async\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m callback_manager_from_settings_or_context(\n\u001b[1;32m    124\u001b[0m     Settings, service_context\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    128\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mretriever,\n\u001b[1;32m    129\u001b[0m     response_synthesizer\u001b[38;5;241m=\u001b[39mresponse_synthesizer,\n\u001b[1;32m    130\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[1;32m    131\u001b[0m     node_postprocessors\u001b[38;5;241m=\u001b[39mnode_postprocessors,\n\u001b[1;32m    132\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/core/response_synthesizers/factory.py:74\u001b[0m, in \u001b[0;36mget_response_synthesizer\u001b[0;34m(llm, prompt_helper, service_context, text_qa_template, refine_template, summary_template, simple_template, response_mode, callback_manager, use_async, streaming, structured_answer_filtering, output_cls, program_factory, verbose)\u001b[0m\n\u001b[1;32m     68\u001b[0m     prompt_helper \u001b[38;5;241m=\u001b[39m service_context\u001b[38;5;241m.\u001b[39mprompt_helper\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     prompt_helper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m         prompt_helper\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39m_prompt_helper\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m PromptHelper\u001b[38;5;241m.\u001b[39mfrom_llm_metadata(\n\u001b[0;32m---> 74\u001b[0m             \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m,\n\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_mode \u001b[38;5;241m==\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mREFINE:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Refine(\n\u001b[1;32m     80\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     81\u001b[0m         callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m         service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[1;32m     92\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/llama_index/llms/azure_inference/base.py:282\u001b[0m, in \u001b[0;36mAzureAICompletionsModel.metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMMetadata:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name:\n\u001b[0;32m--> 282\u001b[0m         model_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model_info:\n\u001b[1;32m    284\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name \u001b[38;5;241m=\u001b[39m model_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/ai/inference/_patch.py:660\u001b[0m, in \u001b[0;36mChatCompletionsClient.get_model_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns information about the AI model.\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \n\u001b[1;32m    655\u001b[0m \u001b[38;5;124;03m:return: ModelInfo. The ModelInfo is compatible with MutableMapping\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m:rtype: ~azure.ai.inference.models.ModelInfo\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03m:raises ~azure.core.exceptions.HttpResponseError:\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_info:\n\u001b[0;32m--> 660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_info\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/Developer/azureai-x-arize/.venv/lib/python3.10/site-packages/azure/ai/inference/_operations/_operations.py:558\u001b[0m, in \u001b[0;36mChatCompletionsClientOperationsMixin._get_model_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m         response\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# Load the body in memory and close the socket\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[0;32m--> 558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[1;32m    561\u001b[0m     deserialized \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39miter_bytes()\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: (no_model_name) No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header.\nCode: no_model_name\nMessage: No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header."
     ]
    }
   ],
   "source": [
    "rag_dataset = dataset_generator.generate_questions_from_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mrag_dataset\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mquery)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rag_dataset[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreference_contexts[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m50\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rag_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Query:\", rag_dataset[1].query)\n",
    "print(\"Context:\", rag_dataset[1].reference_contexts[0][:50], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrag_dataset\u001b[49m\u001b[38;5;241m.\u001b[39msave_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevals/pg_rag_dataset.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rag_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "rag_dataset.save_json(\"evals/pg_rag_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reload them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dataset = LabelledRagDataset.from_json(\"evals/pg_rag_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use evaluations for retrieval\n",
    "\n",
    "__FaithfulnessEvaluator__\n",
    "\n",
    "`FaithfulnessEvaluator` is used to measure if the response from a query engine matches any response nodes. This is useful for measuring if the response has hallucinated.\n",
    "\n",
    "__RelevancyEvaluator__\n",
    "\n",
    "`RelevancyEvaluator` is used to measure if the response and the source nodes match the query. This is useful for measuring if the query was actually answered by the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "from llama_index.core.evaluation import RelevancyEvaluator, FaithfulnessEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case let's use a more powerful model as a judge, being GPT-4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4judge = AzureOpenAI(\n",
    "    deployment=\"gpt-4\",\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_GPT_4_ENDPOINT_URL\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_GPT_4_ENDPOINT_KEY\"],\n",
    "    api_version=\"2023-07-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the evaluators with this LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_evaluator = RelevancyEvaluator(llm=gpt4judge)\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=gpt4judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset of the `query` property only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_eval_queries = [sample.query for sample in rag_dataset[1::2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `rag_dataset[1::2]` retries the odd indexes only, since the dataset contains \"question 1:\" as part of the generation. It probably requires to change the generation template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `BatchEvalRunner` will allow us to run evalutions over all the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = BatchEvalRunner(\n",
    "    {\n",
    "        \"faithfulness\": faithfulness_evaluator,\n",
    "        \"relevancy\": relevancy_evaluator\n",
    "    },\n",
    "    workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m runner\u001b[38;5;241m.\u001b[39maevaluate_queries(\n\u001b[0;32m----> 2\u001b[0m     query_engine, queries\u001b[38;5;241m=\u001b[39mbatch_eval_queries\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_engine' is not defined"
     ]
    }
   ],
   "source": [
    "eval_results = await runner.aevaluate_queries(\n",
    "    query_engine, queries=batch_eval_queries\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m eval_results_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m eval_results_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaithfulness\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mdict\u001b[39m(result) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43meval_results\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaithfulness\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      6\u001b[0m eval_results_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevancy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mdict\u001b[39m(result) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m eval_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevancy\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevals/pg_rag_eval_results_phi3.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_results' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "eval_results_dict = {}\n",
    "eval_results_dict[\"faithfulness\"] = [\n",
    "    dict(result) for result in eval_results[\"faithfulness\"]]\n",
    "eval_results_dict[\"relevancy\"] = [\n",
    "    dict(result) for result in eval_results[\"relevancy\"]]\n",
    "\n",
    "with open(\"evals/pg_rag_eval_results_phi3.json\", \"w\") as f:\n",
    "    json.dump(eval_results_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m faithfulness_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     result\u001b[38;5;241m.\u001b[39mpassing \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43meval_results\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfaithfulness\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfaithfulness\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m relevancy_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m      4\u001b[0m     result\u001b[38;5;241m.\u001b[39mpassing \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelevancy\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelevancy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_results' is not defined"
     ]
    }
   ],
   "source": [
    "faithfulness_score = sum(\n",
    "    result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
    "relevancy_score = sum(\n",
    "    result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faithfulness_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFaithfulness Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfaithfulness_score\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelevancy Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelevancy_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'faithfulness_score' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Faithfulness Score: {faithfulness_score}\")\n",
    "print(f\"Relevancy Score: {relevancy_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
